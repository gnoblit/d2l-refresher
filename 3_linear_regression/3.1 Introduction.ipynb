{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 Basics\n",
    "\n",
    "Interested in some relationship between $\\bold{x}$ and $y$ that is approximately linear (such that $E[Y \\vert X = \\bold{x}]) \\approx \\beta \\bold{x} + \\epsilon$ where $\\beta$ is a set of parameters and $\\epsilon$ some normally distributed error term (i.e. well behaved). Our goal is to find the parameters such that the predictions associated with said parameters, $\\hat{y}$ are close to our labels, $y$. \n",
    "\n",
    "This means we must measure the \"error\" between our predictions and the ground truth labels and also some machinery to permit our parameters to change in a way that said error decreases over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1.2 Loss Function\n",
    "\n",
    "The loss function is how we measure error. It measures the distances between our target, the label, the *real* value for a sample and the predicted value, what our model generates. Therefore, it measures \"error\". One common error function for regression models is:\n",
    "\n",
    "$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2}(\\hat{y}^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "We will want to sum this over all $i$ in our train-set:\n",
    "\n",
    "$$L(\\mathbf{w}, b) = \\frac{1}{2}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2n}\\sum_{i=1}^n\\left( \\mathbf{w}^\\intercal \\mathbf{x}^{(i)} + b - y^{(i)} \\right)^2$$\n",
    "\n",
    "Then we seek the parameters that minimize the total loss:\n",
    "$$\\mathbf{w}^*, b^* = \\argmin_{\\mathbf{w}, b} L(\\mathbf{w}, b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find these optimal parameters? Analytical solutions exist, but there's often no analytical solution when a matrix is not invertible. We can use gradient descent and take derivatives of the loss function with respecte to the parameters of the model. This means we can move the parameters in the direction of the negative gradient to decrease the loss function:\n",
    "\n",
    "$$\\mathbf{w} \\leftarrow \\mathbf{w} = \\frac{\\nu}{\\vert B \\vert} \\partial_\\mathbf{w} l^{(i)}(\\mathbf{w}, b) $$\n",
    "$$b \\leftarrow b = \\frac{\\nu}{\\vert B \\vert} \\partial_b l^{(i)}(\\mathbf{w}, b) $$\n",
    "\n",
    "Here we use stochastic gradient descent where we select a sample $B$ from the overall dataset and iteratively perform parameter updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
